{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "#scvi doesn't work without next 3\n",
    "%pip install pymde\n",
    "%pip install leidenalg\n",
    "%pip install igraph\n",
    "%pip install --quiet gdown\n",
    "%pip install seaborn\n",
    "%pip install --quiet bbknn\n",
    "%pip install pandas\n",
    "%pip install ipywidgets\n",
    "%pip install MulticoreTSNE\n",
    "%pip install scipy\n",
    "%pip install scikit-misc\n",
    "#if you're going to install pytorch or JAX for accelerated computing do it before scvi\n",
    "%pip install scanpy\n",
    "%pip install anndata\n",
    "!pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "%pip install nvidia-cuda-runtime-cu12\n",
    "%pip install nvidia-<library>\n",
    "%pip install -U scvi-tools"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#general\n",
    "import os\n",
    "import glob\n",
    "import tarfile\n",
    "import numpy as np\n",
    "import scanpy as sc\n",
    "import pandas as pd\n",
    "\n",
    "#pyscenic\n",
    "import anndata as ad\n",
    "from anndata.experimental.multi_files import AnnCollection\n",
    "#specific requirements of pyscenic\n",
    "import loompy as lp\n",
    "\n",
    "## This is T-SNE, download has wheel error, could fix... could use UM\n",
    "#from MulticoreTSNE import MulticoreTSNE as TSNE\n",
    "\n",
    "#data vis\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import umap\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scvi\n",
    "import leidenalg\n",
    "import igraph\n",
    "import pymde\n",
    "import torch\n",
    "import tempfile\n",
    "import scvi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "\n",
    "total, used, free = shutil.disk_usage(\"/\")\n",
    "print(f\"Total: {total // (2**30)} GiB\")\n",
    "print(f\"Used: {used // (2**30)} GiB\")\n",
    "print(f\"Free: {free // (2**30)} GiB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to extract tar.gz files\n",
    "def extract_tar_gz(tar_gz_path, extract_dir):\n",
    "    with tarfile.open(tar_gz_path, \"r:gz\") as tar:\n",
    "        tar.extractall(path=extract_dir)\n",
    "\n",
    "# Directory containing your datasets\n",
    "input_dir = \"input/biostudiesarrayE-MTAB-14039\"\n",
    "extracted_dir = \"input/extracted_data\"  # Folder to save the extracted files\n",
    "os.makedirs(extracted_dir, exist_ok=True)\n",
    "\n",
    "# List all top-level files in the input directory\n",
    "tar_gz_files = [f for f in os.listdir(input_dir) if f.endswith('.tar.gz')]\n",
    "\n",
    "# Extract tar.gz files\n",
    "for tar_gz_file in tar_gz_files:\n",
    "    tar_gz_path = os.path.join(input_dir, tar_gz_file)\n",
    "    \n",
    "    # Extract the tar.gz file to the extracted directory\n",
    "    extract_tar_gz(tar_gz_path, extracted_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Directory where the files have been extracted\n",
    "extracted_work_dir = \"input/extracted_data\"  # Directory where you extracted the tar files\n",
    "# List to store directory names under the 'work' directory\n",
    "dataset_dirs = []\n",
    "\n",
    "# Traverse the 'work' directory to find subdirectories\n",
    "for root, dirs, files in os.walk(extracted_work_dir):\n",
    "    # Only consider directories directly under the 'work' directory\n",
    "    if \"work\" in root:\n",
    "        for dir_name in dirs:\n",
    "            path = os.path.join(root, dir_name)\n",
    "            # Append the directory name under 'work'\n",
    "            dataset_dirs.append(path)\n",
    "\n",
    "# Now dataset_dirs contains the names of all subdirectories under 'work'\n",
    "print(\"Dataset directories under 'work':\", dataset_dirs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(dataset_dirs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Docs: https://anndata.readthedocs.io/en/latest/tutorials/notebooks/anncollection.html\n",
    "https://ccbskillssem.github.io/assets/scvi_notebook.html\n",
    "\n",
    "warning: \"If you use join='outer' this fills 0s for sparse data when variables are absent in a batch. Use this with care. Dense data is filled with NaN.\"\n",
    "\n",
    "https://discourse.scverse.org/t/how-to-concatenate-anndata-properly/887/2\n",
    "strongly recommends datacleaning each dataset before concatenating -- assuming that they did it for us"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "hd5a_files_directory = r\"input/hd5a_files\"\n",
    "if not os.path.exists(hd5a_files_directory):\n",
    "    os.makedirs(hd5a_files_directory) \n",
    "adatas_dict = {}\n",
    "for directory in dataset_dirs:\n",
    "    adata = sc.read_10x_mtx(directory)\n",
    "    adata.X = adata.X.tocsr()\n",
    "    #dir_name = directory.split(\"\\\\\")[-1]\n",
    "    dir_name = os.path.basename(directory) #platform independent\n",
    "    h5ad_filename = f\"{dir_name}.h5ad\"\n",
    "\n",
    "    h5ad_path = os.path.join(hd5a_files_directory, h5ad_filename)\n",
    "    adata.write(h5ad_path)\n",
    "    adatas_dict[dir_name] = h5ad_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adatas_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_directory = r\"output/\"\n",
    "combined_on_disk_name = os.path.join(output_directory, 'data_combined_on_disk.h5ad') \n",
    "#os.remove(combined_on_disk_name)\n",
    "ad.experimental.concat_on_disk(\n",
    "    adatas_dict,          # List of paths to the individual .h5ad files\n",
    "    combined_on_disk_name,  # Path to the combined output file\n",
    "    label='dataset'      # Optional: label for the datasets\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##METHOD USING BATCHING -- broken???\n",
    "# # Directory where the files have been extracted\n",
    "# extracted_work_dir = \"input/extracted_data\"  # Directory where you extracted the tar files\n",
    "# # List to store directory names under the 'work' directory\n",
    "# dataset_dirs = []\n",
    "\n",
    "# # Traverse the 'work' directory to find subdirectories\n",
    "# for root, dirs, files in os.walk(extracted_work_dir):\n",
    "#     # Only consider directories directly under the 'work' directory\n",
    "#     if \"work\" in root:\n",
    "#         for dir_name in dirs:\n",
    "#             path = os.path.join(root, dir_name)\n",
    "#             # Append the directory name under 'work'\n",
    "#             dataset_dirs.append(path)\n",
    "\n",
    "# # Now dataset_dirs contains the names of all subdirectories under 'work'\n",
    "# print(\"Dataset directories under 'work':\", dataset_dirs)\n",
    "\n",
    "\n",
    "\n",
    "# # List to store AnnData objects for merging\n",
    "# adatas = []\n",
    "# batch_size = 5  # Adjust based on available memory\n",
    "# for i in range(0, len(dataset_dirs), batch_size):\n",
    "#     # Get the current batch of files\n",
    "#     batch_files = dataset_dirs[i:i+batch_size]\n",
    "    \n",
    "#     # Load the current batch of datasets\n",
    "#     batch_adatas = []\n",
    "#     for mtx_file in batch_files:\n",
    "#         print(mtx_file)\n",
    "#         # Assuming each batch is loaded from corresponding directories\n",
    "#         adata = sc.read_10x_mtx(mtx_file)\n",
    "#         batch_adatas.append(adata)\n",
    "    \n",
    "#     # Concatenate the current batch\n",
    "#     if adatas:\n",
    "#         adatas[0] = ad.concat([adatas[0]] + batch_adatas)\n",
    "#         adatas[0].obs_names_make_unique() \n",
    "#     else:\n",
    "#         adatas = batch_adatas\n",
    "\n",
    "#     # Free memory by clearing the batch Adatas\n",
    "#     batch_adatas = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load in data -- OLAF's VERSION. Garrett Depending on what you did you made need to make a different block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata = sc.read(\"output/data_combined_on_disk.h5ad\")\n",
    "combined_adata.obs_names_make_unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.obs.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "#read text file into pandas DataFrame\n",
    "df = pd.read_csv('input/E-MTAB-14039.sdrf.txt', sep='\\t', comment='#', on_bad_lines='skip')\n",
    "#print(df.columns)\n",
    "healthy_tissue = df[df['Characteristics[disease]'] == 'normal']['Source Name']\n",
    "\n",
    "print(len(healthy_tissue))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#THIS WORKS BUT IS TOO SMALL\n",
    "#combined_adata = combined_adata[~combined_adata.obs['dataset'].str.contains('Endo', na=False)]\n",
    "combined_adata = combined_adata[combined_adata.obs['dataset'].isin(healthy_tissue)]\n",
    "combined_adata.obs['dataset']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from scipy import sparse\n",
    "\n",
    "#sparse_X = sparse.csr_matrix(combined_adata.X)\n",
    "#combined_adata.X = sparse_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "combined_adata.var.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_adata.var_names)\n",
    "combined_adata.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.obs\n",
    "#output_file = \"output/integrated_data.h5ad\"\n",
    "#combined_adata.write(output_file) # might just be failing because it already exists"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.var['MT'] = combined_adata.var_names.str.startswith('MT-')  # annotate the group of mitochondrial genes as 'mt'\n",
    "sc.pp.calculate_qc_metrics(combined_adata, qc_vars=['MT'], percent_top=None, log1p=False, inplace=True)\n",
    "\n",
    "\n",
    "combined_adata.obs.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.violin(\n",
    "    combined_adata, \n",
    "    [\n",
    "     'n_genes_by_counts', \n",
    "     'total_counts', \n",
    "     'pct_counts_MT'\n",
    "     ],\n",
    "    multi_panel=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.scatter(combined_adata, \"total_counts\", \"n_genes_by_counts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.filter_cells(combined_adata, min_genes=100)\n",
    "sc.pp.filter_genes(combined_adata, min_cells=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#skipping, I don't think doublets make sense to detect with dataset that has already been processed and integrated.\n",
    "#sc.pp.scrublet(combined_adata, batch_key=\"sample\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os.environ[\"KMP_DUPLICATE_LIB_OK\"]=\"TRUE\"\n",
    "#combined_adata = sc.read(\"output/integrated_data.h5ad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.layers[\"counts\"] = combined_adata.X.copy()\n",
    "sc.pp.normalize_total(combined_adata, target_sum=1e4) \n",
    "sc.pp.log1p(combined_adata) \n",
    "combined_adata.raw = combined_adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.highly_variable_genes(\n",
    "    combined_adata,\n",
    "    n_top_genes=4000,\n",
    "    # subset=True, # to automatically subset to the 4000 genes\n",
    "    layer=\"counts\",\n",
    "    flavor=\"seurat_v3\"\n",
    ")\n",
    "sc.pl.highly_variable_genes(combined_adata, log=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata = combined_adata[:, combined_adata.var.highly_variable].copy()\n",
    "combined_adata.raw.to_adata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.var.columns\n",
    "\n",
    "# The .var attribute stores metadata related to variables (typically genes). \n",
    "#view with specific with\n",
    "## adata.var['gene_type'].head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.obs.columns\n",
    "#The .obs attribute stores metadata related to the observations (cells). To see the available metadata columns:\n",
    "#view with \n",
    "## adata.obs['cell_type'].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata.uns.keys()\n",
    "#The .uns attribute contains unstructured annotations that may include\n",
    "#  clustering results, embeddings, or other metadata that doesn’t fit neatly into .obs or .var. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_adata.layers.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Standard Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pp.scale(combined_adata) # z normalize the columns (genes)\n",
    "sc.tl.pca(combined_adata)\n",
    "\n",
    "\n",
    "combined_adata.obsm[\"X_pca\"]\n",
    "sc.pp.neighbors(combined_adata) # compute nearest neighbors\n",
    "sc.tl.umap(combined_adata)\n",
    "sc.pl.umap(combined_adata, color=\"dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF THE CLUSTERS ARE BY BATCH:: ADD THIS CODE:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import scanpy.external as sce\n",
    "sce.pp.bbknn(combined_adata, batch_key=\"dataset\")\n",
    "sc.tl.umap(combined_adata)\n",
    "sc.pl.umap(combined_adata, color=[\"dataset\"], ncols=1)\n",
    "#don't know what this 'computation' warning is. No 'computation' parameter in bbknn or umap\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#combined_adata.write(\"output/combined_with_umap_no_scvi.h5ad\")\n",
    "combined_adata = sc.read(\"output/combined_with_umap_no_scvi.h5ad\")\n",
    "combined_adata.obs.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IF IT STILL DOESN'T WORK THEN REDO THE LOAD IN AND USE SCIVI\n",
    "https://ccbskillssem.github.io/assets/scvi_notebook.html\n",
    "\n",
    "https://www.youtube.com/watch?v=EKTg9NV5hEA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc.set_figure_params(figsize=(6, 6), frameon=False)\n",
    "sns.set_theme()\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "#save_dir = tempfile.TemporaryDirectory()\n",
    "save_dir = 'output/scvi_checkpoints' \n",
    "#%config InlineBackend.print_figure_kwargs={\"facecolor\": \"w\"}\n",
    "#%config InlineBackend.figure_format=\"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "print(f\"Is CUDA supported by this system? {torch.cuda.is_available()}\")\n",
    "print(f\"CUDA version: {torch.version.cuda}\")\n",
    "\n",
    "# Storing ID of current CUDA device\n",
    "cuda_id = torch.cuda.current_device()\n",
    "print(f\"ID of current CUDA device:{torch.cuda.current_device()}\")\n",
    "\t\n",
    "print(f\"Name of current CUDA device:{torch.cuda.get_device_name(cuda_id)}\")\n",
    "\n",
    "#Garrett if this is false for you -- \n",
    "## run this in powershell (assuming windows/pip/python): pip3 install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu124\n",
    "## OR just comment out the next block rewrite it without the accelerator. It will take longer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scvi.settings.seed = 0\n",
    "sc.set_figure_params(figsize=(6, 6), frameon=False)\n",
    "sns.set_theme()\n",
    "torch.set_float32_matmul_precision(\"high\")\n",
    "save_dir = tempfile.TemporaryDirectory()\n",
    "\n",
    "%config InlineBackend.print_figure_kwargs={\"facecolor\": \"w\"}\n",
    "%config InlineBackend.figure_format=\"retina\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print(scvi.__version__)\n",
    "# Set up the AnnData object (combined_adata)\n",
    "scvi.model.SCVI.setup_anndata(combined_adata, layer=\"counts\", batch_key='dataset')\n",
    "\n",
    "# Initialize the SCVI model\n",
    "model = scvi.model.SCVI(combined_adata) #, n_layers=2, n_latent=30, gene_likelihood=\"nb\"\n",
    "\n",
    "# Set the number of workers for data loading\n",
    "scvi.settings.dl_num_workers = 0 #default is 0, gives warnings to raise but then gives iter(combined_loader) errors\n",
    "# Train the model, specifying the accelerator (GPU or CPU)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(str(combined_adata))\n",
    "print(combined_adata.layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train() \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCVI_LATENT_KEY = \"X_scVI\"\n",
    "combined_adata.obsm[SCVI_LATENT_KEY] = model.get_latent_representation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc.pp.neighbors(combined_adata, use_rep=SCVI_LATENT_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "sc.tl.leiden(combined_adata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SCVI_MDE_KEY = \"X_scVI_MDE\"\n",
    "combined_adata.obsm[SCVI_MDE_KEY] = scvi.model.utils.mde(combined_adata.obsm[SCVI_LATENT_KEY]) #last run was with , accelerator=\"cpu\" but I removed this so that it can use gpu, return if breaks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.pl.embedding(\n",
    "    combined_adata,\n",
    "    basis=SCVI_MDE_KEY,\n",
    "    color=[\"dataset\", \"leiden\"],\n",
    "    frameon=False,\n",
    "    ncols=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file = \"output/scvi_integrated.h5ad\"\n",
    "#combined_adata.write(output_file)\n",
    "combined_adata = sc.read(output_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SCENIC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scvi and and pySCENIC may be incompatible.\n",
    "pySCENIC requires numpy 2.0> and scvi requires numpy <2.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip uninstall numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting dask[complete]\n",
      "  Using cached dask-2024.11.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: click>=8.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (8.1.7)\n",
      "Requirement already satisfied: cloudpickle>=3.0.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (3.1.0)\n",
      "Requirement already satisfied: fsspec>=2021.09.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (1.4.2)\n",
      "Requirement already satisfied: pyyaml>=5.3.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (6.0.2)\n",
      "Requirement already satisfied: toolz>=0.10.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: pyarrow>=14.0.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (18.0.0)\n",
      "Requirement already satisfied: lz4>=4.3.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (4.3.3)\n",
      "Requirement already satisfied: colorama in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from click>=8.1->dask[complete]) (0.4.6)\n",
      "Requirement already satisfied: locket in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from partd>=1.4.0->dask[complete]) (1.0.0)\n",
      "Requirement already satisfied: numpy>=1.24 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (2.0.2)\n",
      "Requirement already satisfied: pandas>=2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (2.2.3)\n",
      "Collecting dask-expr<1.2,>=1.1 (from dask[complete])\n",
      "  Using cached dask_expr-1.1.19-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: bokeh>=3.1.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (3.6.1)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]) (3.1.4)\n",
      "Collecting distributed==2024.11.2 (from dask[complete])\n",
      "  Using cached distributed-2024.11.2-py3-none-any.whl.metadata (3.3 kB)\n",
      "Requirement already satisfied: msgpack>=1.0.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.8.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (6.1.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.26.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (2.2.3)\n",
      "Requirement already satisfied: zict>=3.0.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed==2024.11.2->dask[complete]) (3.0.0)\n",
      "Requirement already satisfied: contourpy>=1.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (1.3.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (11.0.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from bokeh>=3.1.0->dask[complete]) (2024.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from jinja2>=2.10.3->dask[complete]) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pandas>=2.0->dask[complete]) (2024.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=2.0->dask[complete]) (1.16.0)\n",
      "Using cached dask-2024.11.2-py3-none-any.whl (1.3 MB)\n",
      "Using cached distributed-2024.11.2-py3-none-any.whl (1.0 MB)\n",
      "Using cached dask_expr-1.1.19-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: dask, distributed, dask-expr\n",
      "  Attempting uninstall: distributed\n",
      "    Found existing installation: distributed 2024.2.1\n",
      "    Uninstalling distributed-2024.2.1:\n",
      "      Successfully uninstalled distributed-2024.2.1\n",
      "Successfully installed dask-2024.11.2 dask-expr-1.1.19 distributed-2024.11.2\n"
     ]
    }
   ],
   "source": [
    "!pip install dask-expr==0.5.3 distributed==2024.2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/aertslab/pySCENIC.git\n",
      "  Cloning https://github.com/aertslab/pySCENIC.git to c:\\users\\olaf\\appdata\\local\\temp\\pip-req-build-naj_diy4\n",
      "  Resolved https://github.com/aertslab/pySCENIC.git to commit d2309fed83759a0e66e0a995ef3d2f40bf4d6c1a\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: ctxcore>=0.2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (0.2.0)\n",
      "Requirement already satisfied: cytoolz in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (1.0.0)\n",
      "Requirement already satisfied: multiprocessing_on_dill in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (3.5.0a4)\n",
      "Requirement already satisfied: llvmlite in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (0.43.0)\n",
      "Requirement already satisfied: numba>=0.51.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (0.60.0)\n",
      "Requirement already satisfied: attrs in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (24.2.0)\n",
      "Requirement already satisfied: frozendict in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2.4.6)\n",
      "Requirement already satisfied: numpy in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2.0.2)\n",
      "Requirement already satisfied: pandas>=1.3.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2.2.3)\n",
      "Requirement already satisfied: numexpr in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2.10.1)\n",
      "Requirement already satisfied: cloudpickle in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (3.1.0)\n",
      "Collecting dask (from pyscenic==0.12.1+8.gd2309fe)\n",
      "  Using cached dask-2024.11.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: distributed in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2024.2.1)\n",
      "Requirement already satisfied: arboreto>=0.1.6 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (0.1.6)\n",
      "Requirement already satisfied: boltons in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (24.1.0)\n",
      "Requirement already satisfied: setuptools in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (75.5.0)\n",
      "Requirement already satisfied: pyyaml in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (6.0.2)\n",
      "Requirement already satisfied: tqdm in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (4.67.0)\n",
      "Requirement already satisfied: interlap in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (0.2.7)\n",
      "Requirement already satisfied: umap-learn in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (0.5.7)\n",
      "Requirement already satisfied: loompy in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (3.0.7)\n",
      "Requirement already satisfied: networkx in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (3.4.2)\n",
      "Requirement already satisfied: scipy in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (1.14.1)\n",
      "Requirement already satisfied: fsspec in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2024.10.0)\n",
      "Requirement already satisfied: requests in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (2.32.3)\n",
      "Requirement already satisfied: aiohttp in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (3.11.2)\n",
      "Requirement already satisfied: scikit-learn>=0.22.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pyscenic==0.12.1+8.gd2309fe) (1.5.2)\n",
      "Requirement already satisfied: pyarrow>=8.0.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from ctxcore>=0.2.0->pyscenic==0.12.1+8.gd2309fe) (18.0.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pandas>=1.3.5->pyscenic==0.12.1+8.gd2309fe) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pandas>=1.3.5->pyscenic==0.12.1+8.gd2309fe) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from pandas>=1.3.5->pyscenic==0.12.1+8.gd2309fe) (2024.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from scikit-learn>=0.22.2->pyscenic==0.12.1+8.gd2309fe) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from scikit-learn>=0.22.2->pyscenic==0.12.1+8.gd2309fe) (3.5.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from aiohttp->pyscenic==0.12.1+8.gd2309fe) (2.4.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from aiohttp->pyscenic==0.12.1+8.gd2309fe) (1.3.1)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from aiohttp->pyscenic==0.12.1+8.gd2309fe) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from aiohttp->pyscenic==0.12.1+8.gd2309fe) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from aiohttp->pyscenic==0.12.1+8.gd2309fe) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from aiohttp->pyscenic==0.12.1+8.gd2309fe) (1.17.2)\n",
      "Requirement already satisfied: toolz>=0.8.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from cytoolz->pyscenic==0.12.1+8.gd2309fe) (1.0.0)\n",
      "Requirement already satisfied: click>=8.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask->pyscenic==0.12.1+8.gd2309fe) (8.1.7)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask->pyscenic==0.12.1+8.gd2309fe) (24.2)\n",
      "Requirement already satisfied: partd>=1.4.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask->pyscenic==0.12.1+8.gd2309fe) (1.4.2)\n",
      "  Using cached dask-2024.2.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: jinja2>=2.10.3 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (3.1.4)\n",
      "Requirement already satisfied: locket>=1.0.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (1.0.0)\n",
      "Requirement already satisfied: msgpack>=1.0.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (1.1.0)\n",
      "Requirement already satisfied: psutil>=5.7.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (6.1.0)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (2.4.0)\n",
      "Requirement already satisfied: tblib>=1.6.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (3.0.0)\n",
      "Requirement already satisfied: tornado>=6.0.4 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (6.4.1)\n",
      "Requirement already satisfied: urllib3>=1.24.3 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (2.2.3)\n",
      "Requirement already satisfied: zict>=3.0.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from distributed->pyscenic==0.12.1+8.gd2309fe) (3.0.0)\n",
      "Requirement already satisfied: importlib-metadata>=4.13.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask->pyscenic==0.12.1+8.gd2309fe) (8.5.0)\n",
      "Requirement already satisfied: h5py in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from loompy->pyscenic==0.12.1+8.gd2309fe) (3.12.1)\n",
      "Requirement already satisfied: numpy-groupies in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from loompy->pyscenic==0.12.1+8.gd2309fe) (0.11.2)\n",
      "Requirement already satisfied: dill in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from multiprocessing_on_dill->pyscenic==0.12.1+8.gd2309fe) (0.3.9)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from requests->pyscenic==0.12.1+8.gd2309fe) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from requests->pyscenic==0.12.1+8.gd2309fe) (3.10)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from requests->pyscenic==0.12.1+8.gd2309fe) (2024.8.30)\n",
      "Requirement already satisfied: colorama in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from tqdm->pyscenic==0.12.1+8.gd2309fe) (0.4.6)\n",
      "Requirement already satisfied: pynndescent>=0.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from umap-learn->pyscenic==0.12.1+8.gd2309fe) (0.5.13)\n",
      "Requirement already satisfied: zipp>=3.20 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from importlib-metadata>=4.13.0->dask->pyscenic==0.12.1+8.gd2309fe) (3.21.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from jinja2>=2.10.3->distributed->pyscenic==0.12.1+8.gd2309fe) (3.0.2)\n",
      "Requirement already satisfied: six>=1.5 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.3.5->pyscenic==0.12.1+8.gd2309fe) (1.16.0)\n",
      "INFO: pip is looking at multiple versions of dask[complete] to determine which version is compatible with other requirements. This could take a while.\n",
      "Collecting dask[complete] (from arboreto>=0.1.6->pyscenic==0.12.1+8.gd2309fe)\n",
      "  Downloading dask-2024.11.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading dask-2024.10.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading dask-2024.9.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading dask-2024.9.0-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading dask-2024.8.2-py3-none-any.whl.metadata (3.7 kB)\n",
      "  Downloading dask-2024.8.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.8.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: pip is still looking at multiple versions of dask[complete] to determine which version is compatible with other requirements. This could take a while.\n",
      "  Downloading dask-2024.7.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.7.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.6.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.6.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.6.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
      "  Downloading dask-2024.5.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.5.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.5.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.4.2-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.4.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.4.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.3.1-py3-none-any.whl.metadata (3.8 kB)\n",
      "  Downloading dask-2024.3.0-py3-none-any.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: pyarrow-hotfix in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]->arboreto>=0.1.6->pyscenic==0.12.1+8.gd2309fe) (0.6)\n",
      "Requirement already satisfied: lz4>=4.3.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask[complete]->arboreto>=0.1.6->pyscenic==0.12.1+8.gd2309fe) (4.3.3)\n",
      "Requirement already satisfied: bokeh>=2.4.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from dask->pyscenic==0.12.1+8.gd2309fe) (3.6.1)\n",
      "Requirement already satisfied: contourpy>=1.2 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from bokeh>=2.4.2->dask->pyscenic==0.12.1+8.gd2309fe) (1.3.1)\n",
      "Requirement already satisfied: pillow>=7.1.0 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from bokeh>=2.4.2->dask->pyscenic==0.12.1+8.gd2309fe) (11.0.0)\n",
      "Requirement already satisfied: xyzservices>=2021.09.1 in d:\\coding\\comp_bio\\endometrial_analysis\\.venv\\lib\\site-packages (from bokeh>=2.4.2->dask->pyscenic==0.12.1+8.gd2309fe) (2024.9.0)\n",
      "Using cached dask-2024.2.1-py3-none-any.whl (1.2 MB)\n",
      "Installing collected packages: dask\n",
      "Successfully installed dask-2024.2.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  Running command git clone --filter=blob:none --quiet https://github.com/aertslab/pySCENIC.git 'C:\\Users\\Olaf\\AppData\\Local\\Temp\\pip-req-build-naj_diy4'\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/aertslab/pySCENIC.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask\n",
    "import dask_expr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from dask.diagnostics import ProgressBar\n",
    "\n",
    "from arboreto.utils import load_tf_names\n",
    "from arboreto.algo import grnboost2\n",
    "\n",
    "from ctxcore.rnkdb import FeatherRankingDatabase as RankingDatabase\n",
    "from pyscenic.utils import modules_from_adjacencies, load_motifs\n",
    "from pyscenic.prune import prune2df, df2regulons\n",
    "from pyscenic.aucell import aucell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Olaf\\AppData\\Local\\Temp\\ipykernel_13708\\1954661800.py:6: DtypeWarning: Columns (4) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_motifs_hgnc = pd.read_csv(MOTIFS_HGNC_FNAME, sep='\\t')\n"
     ]
    }
   ],
   "source": [
    "BASEFOLDER_NAME = 'Input/tf_motifs/'\n",
    "\n",
    "MOTIFS_HGNC_FNAME = os.path.join(BASEFOLDER_NAME, 'motifs-v9-nr.hgnc-m0.001-o0.0.tbl')\n",
    "OUT_TFS_HGNC_FNAME = os.path.join(BASEFOLDER_NAME, 'hs_hgnc_tfs.txt')\n",
    "\n",
    "df_motifs_hgnc = pd.read_csv(MOTIFS_HGNC_FNAME, sep='\\t')\n",
    "hs_tfs = df_motifs_hgnc.gene_name.unique()\n",
    "with open(OUT_TFS_HGNC_FNAME, 'wt') as f:\n",
    "    f.write('\\n'.join(hs_tfs) + '\\n')\n",
    "len(hs_tfs)\n",
    "hs_tfs_list = hs_tfs.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download tf motifs from \n",
    "wget https://resources.aertslab.org/cistarget/motif2tf/motifs-v9-nr.hgnc-m0.001-o0.0.tbl\n",
    "https://github.com/aertslab/pySCENIC/blob/master/notebooks/pySCENIC%20-%20List%20of%20Transcription%20Factors.ipynb\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download database from https://resources.aertslab.org/cistarget/databases/homo_sapiens/hg38/refseq_r80/mc_v10_clust/gene_based/ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DATABASE_FOLDER = \"input/motif_database\"\n",
    "# RANKING_DBS_FNAMES = list(map(lambda fn: os.path.join(DATABASE_FOLDER, fn),\n",
    "#                        ['hg38_10kbp_up_10kbp_down_full_tx_v10_clust.genes_vs_motifs.rankings.feather',\n",
    "#                        'hg38_10kbp_up_10kbp_down_full_tx_v10_clust.genes_vs_motifs.scores.feather',\n",
    "#                         'hg38_500bp_up_100bp_down_full_tx_v10_clust.genes_vs_motifs.rankings.feather',\n",
    "#                         'hg38_500bp_up_100bp_down_full_tx_v10_clust.genes_vs_motifs.scores.feather']))\n",
    "# RANKING_DBS_FNAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[FeatherRankingDatabase(name=\"hg38_10kbp_up_10kbp_down_full_tx_v10_clust.genes_vs_motifs.rankings\"),\n",
       " FeatherRankingDatabase(name=\"hg38_500bp_up_100bp_down_full_tx_v10_clust.genes_vs_motifs.rankings\")]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "DATABASE_FOLDER = \"input/motif_database/\"\n",
    "DATABASES_GLOB = os.path.join(DATABASE_FOLDER, \"hg38_*.rankings.feather\")\n",
    "\n",
    "db_fnames = glob.glob(DATABASES_GLOB)\n",
    "def name(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "dbs = [RankingDatabase(fname=fname, name=name(fname)) for fname in db_fnames]\n",
    "dbs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_adata = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                dataset  n_genes_by_counts  total_counts  \\\n",
      "AAACCCAAGGCTTTCA-1  FRZFRESH_GX25_ES345               3737       15451.0   \n",
      "AAACCCACAAGCTCTA-1  FRZFRESH_GX25_ES345               2225        4539.0   \n",
      "AAACCCAGTACGGTTT-1  FRZFRESH_GX25_ES345               3379       11695.0   \n",
      "AAACCCAGTAGGACCA-1  FRZFRESH_GX25_ES345               2307        6950.0   \n",
      "AAACCCATCCCGAGGT-1  FRZFRESH_GX25_ES345               2487        7820.0   \n",
      "\n",
      "                    total_counts_MT  pct_counts_MT  n_genes  _scvi_batch  \\\n",
      "AAACCCAAGGCTTTCA-1           1092.0       7.067504     3737            0   \n",
      "AAACCCACAAGCTCTA-1            212.0       4.670632     2225            0   \n",
      "AAACCCAGTACGGTTT-1           1544.0      13.202223     3379            0   \n",
      "AAACCCAGTAGGACCA-1            700.0      10.071942     2307            0   \n",
      "AAACCCATCCCGAGGT-1            658.0       8.414322     2487            0   \n",
      "\n",
      "                    _scvi_labels leiden  \n",
      "AAACCCAAGGCTTTCA-1             0     12  \n",
      "AAACCCACAAGCTCTA-1             0      9  \n",
      "AAACCCAGTACGGTTT-1             0      8  \n",
      "AAACCCAGTAGGACCA-1             0      2  \n",
      "AAACCCATCCCGAGGT-1             0      2  \n"
     ]
    }
   ],
   "source": [
    "print(combined_adata.obs.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_matrix = pd.DataFrame(combined_adata.X, index=combined_adata.obs_names, columns=combined_adata.var_names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HES4</th>\n",
       "      <th>ISG15</th>\n",
       "      <th>AURKAIP1</th>\n",
       "      <th>VWA1</th>\n",
       "      <th>CFAP74</th>\n",
       "      <th>HES5</th>\n",
       "      <th>PRDM16-DT</th>\n",
       "      <th>PRDM16</th>\n",
       "      <th>AL365255.1</th>\n",
       "      <th>PARK7</th>\n",
       "      <th>...</th>\n",
       "      <th>HSFX3</th>\n",
       "      <th>HMGB3</th>\n",
       "      <th>CETN2</th>\n",
       "      <th>BGN</th>\n",
       "      <th>L1CAM</th>\n",
       "      <th>AVPR2</th>\n",
       "      <th>CTAG2</th>\n",
       "      <th>MT-ND4L</th>\n",
       "      <th>MT-ND4</th>\n",
       "      <th>AC233755.2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>AAACCCAAGGCTTTCA-1</th>\n",
       "      <td>2.904978</td>\n",
       "      <td>1.548665</td>\n",
       "      <td>0.335995</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.109324</td>\n",
       "      <td>-0.078842</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>2.404145</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>0.980200</td>\n",
       "      <td>-0.458562</td>\n",
       "      <td>3.734538</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>-0.064821</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.422837</td>\n",
       "      <td>1.218515</td>\n",
       "      <td>-0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCACAAGCTCTA-1</th>\n",
       "      <td>-0.492316</td>\n",
       "      <td>-0.449396</td>\n",
       "      <td>-0.607010</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.109324</td>\n",
       "      <td>-0.078842</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>1.542172</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>5.213290</td>\n",
       "      <td>-0.458562</td>\n",
       "      <td>-0.321802</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>-0.064821</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.310543</td>\n",
       "      <td>0.812029</td>\n",
       "      <td>-0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCAGTACGGTTT-1</th>\n",
       "      <td>0.735127</td>\n",
       "      <td>-0.449396</td>\n",
       "      <td>3.284012</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.109324</td>\n",
       "      <td>-0.078842</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>1.716625</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>2.357527</td>\n",
       "      <td>1.127928</td>\n",
       "      <td>-0.321802</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>-0.064821</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.416312</td>\n",
       "      <td>1.565777</td>\n",
       "      <td>-0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCAGTAGGACCA-1</th>\n",
       "      <td>-0.492316</td>\n",
       "      <td>1.695543</td>\n",
       "      <td>-0.607010</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.109324</td>\n",
       "      <td>-0.078842</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>2.119331</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>-0.400349</td>\n",
       "      <td>-0.458562</td>\n",
       "      <td>-0.321802</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>-0.064821</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>0.041307</td>\n",
       "      <td>1.450454</td>\n",
       "      <td>-0.004653</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>AAACCCATCCCGAGGT-1</th>\n",
       "      <td>-0.492316</td>\n",
       "      <td>-0.449396</td>\n",
       "      <td>1.790876</td>\n",
       "      <td>-0.295039</td>\n",
       "      <td>-0.109324</td>\n",
       "      <td>-0.078842</td>\n",
       "      <td>-0.058692</td>\n",
       "      <td>-0.115553</td>\n",
       "      <td>-0.052442</td>\n",
       "      <td>2.388958</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.015287</td>\n",
       "      <td>-0.400349</td>\n",
       "      <td>1.656099</td>\n",
       "      <td>-0.321802</td>\n",
       "      <td>-0.026529</td>\n",
       "      <td>-0.064821</td>\n",
       "      <td>-0.027791</td>\n",
       "      <td>-0.025744</td>\n",
       "      <td>1.295379</td>\n",
       "      <td>-0.004653</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 4000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        HES4     ISG15  AURKAIP1      VWA1    CFAP74  \\\n",
       "AAACCCAAGGCTTTCA-1  2.904978  1.548665  0.335995 -0.295039 -0.109324   \n",
       "AAACCCACAAGCTCTA-1 -0.492316 -0.449396 -0.607010 -0.295039 -0.109324   \n",
       "AAACCCAGTACGGTTT-1  0.735127 -0.449396  3.284012 -0.295039 -0.109324   \n",
       "AAACCCAGTAGGACCA-1 -0.492316  1.695543 -0.607010 -0.295039 -0.109324   \n",
       "AAACCCATCCCGAGGT-1 -0.492316 -0.449396  1.790876 -0.295039 -0.109324   \n",
       "\n",
       "                        HES5  PRDM16-DT    PRDM16  AL365255.1     PARK7  ...  \\\n",
       "AAACCCAAGGCTTTCA-1 -0.078842  -0.058692 -0.115553   -0.052442  2.404145  ...   \n",
       "AAACCCACAAGCTCTA-1 -0.078842  -0.058692 -0.115553   -0.052442  1.542172  ...   \n",
       "AAACCCAGTACGGTTT-1 -0.078842  -0.058692 -0.115553   -0.052442  1.716625  ...   \n",
       "AAACCCAGTAGGACCA-1 -0.078842  -0.058692 -0.115553   -0.052442  2.119331  ...   \n",
       "AAACCCATCCCGAGGT-1 -0.078842  -0.058692 -0.115553   -0.052442  2.388958  ...   \n",
       "\n",
       "                       HSFX3     HMGB3     CETN2       BGN     L1CAM  \\\n",
       "AAACCCAAGGCTTTCA-1 -0.015287  0.980200 -0.458562  3.734538 -0.026529   \n",
       "AAACCCACAAGCTCTA-1 -0.015287  5.213290 -0.458562 -0.321802 -0.026529   \n",
       "AAACCCAGTACGGTTT-1 -0.015287  2.357527  1.127928 -0.321802 -0.026529   \n",
       "AAACCCAGTAGGACCA-1 -0.015287 -0.400349 -0.458562 -0.321802 -0.026529   \n",
       "AAACCCATCCCGAGGT-1 -0.015287 -0.400349  1.656099 -0.321802 -0.026529   \n",
       "\n",
       "                       AVPR2     CTAG2   MT-ND4L    MT-ND4  AC233755.2  \n",
       "AAACCCAAGGCTTTCA-1 -0.064821 -0.027791  0.422837  1.218515   -0.004653  \n",
       "AAACCCACAAGCTCTA-1 -0.064821 -0.027791  0.310543  0.812029   -0.004653  \n",
       "AAACCCAGTACGGTTT-1 -0.064821 -0.027791  0.416312  1.565777   -0.004653  \n",
       "AAACCCAGTAGGACCA-1 -0.064821 -0.027791  0.041307  1.450454   -0.004653  \n",
       "AAACCCATCCCGAGGT-1 -0.064821 -0.027791 -0.025744  1.295379   -0.004653  \n",
       "\n",
       "[5 rows x 4000 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ex_matrix.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from arboreto.algo import grnboost2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "dask_ex_matrix = dd.from_pandas(ex_matrix, npartitions=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "preparing dask client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Coding\\comp_bio\\endometrial_analysis\\.venv\\Lib\\site-packages\\distributed\\client.py:1612: VersionMismatchWarning: Mismatched versions found\n",
      "\n",
      "+-------------+-----------+-----------+----------+\n",
      "| Package     | Client    | Scheduler | Workers  |\n",
      "+-------------+-----------+-----------+----------+\n",
      "| dask        | 2024.11.2 | 2024.11.2 | 2024.2.1 |\n",
      "| distributed | 2024.11.2 | 2024.11.2 | 2024.2.1 |\n",
      "+-------------+-----------+-----------+----------+\n",
      "  # Here, `type` may be a str if actual type failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "parsing input\n",
      "creating dask graph\n",
      "shutting down client and local cluster\n",
      "finished\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "Must supply at least one delayed object",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[54], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Call grnboost2 with the extracted matrix\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m adjacencies \u001b[38;5;241m=\u001b[39m \u001b[43mgrnboost2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mex_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhs_tfs_list\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_or_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\comp_bio\\endometrial_analysis\\.venv\\Lib\\site-packages\\arboreto\\algo.py:39\u001b[0m, in \u001b[0;36mgrnboost2\u001b[1;34m(expression_data, gene_names, tf_names, client_or_address, early_stop_window_length, limit, seed, verbose)\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrnboost2\u001b[39m(expression_data,\n\u001b[0;32m     11\u001b[0m               gene_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     12\u001b[0m               tf_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     16\u001b[0m               seed\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m     17\u001b[0m               verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     18\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;124;03m    Launch arboreto with [GRNBoost2] profile.\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[38;5;124;03m    :return: a pandas DataFrame['TF', 'target', 'importance'] representing the inferred gene regulatory links.\u001b[39;00m\n\u001b[0;32m     37\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdiy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mexpression_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregressor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mGBM\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mSGBM_KWARGS\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     40\u001b[0m \u001b[43m               \u001b[49m\u001b[43mgene_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgene_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtf_names\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtf_names\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclient_or_address\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient_or_address\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[43m               \u001b[49m\u001b[43mearly_stop_window_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_window_length\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Coding\\comp_bio\\endometrial_analysis\\.venv\\Lib\\site-packages\\arboreto\\algo.py:120\u001b[0m, in \u001b[0;36mdiy\u001b[1;34m(expression_data, regressor_type, regressor_kwargs, gene_names, tf_names, client_or_address, early_stop_window_length, limit, seed, verbose)\u001b[0m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    118\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcreating dask graph\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m--> 120\u001b[0m graph \u001b[38;5;241m=\u001b[39m \u001b[43mcreate_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexpression_matrix\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    121\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mgene_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    122\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mtf_names\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    123\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mclient\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclient\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    124\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mregressor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    125\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mregressor_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mregressor_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    126\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mearly_stop_window_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mearly_stop_window_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mlimit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlimit\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    128\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mseed\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mseed\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    130\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[0;32m    131\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m partitions\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(graph\u001b[38;5;241m.\u001b[39mnpartitions))\n",
      "File \u001b[1;32md:\\Coding\\comp_bio\\endometrial_analysis\\.venv\\Lib\\site-packages\\arboreto\\core.py:450\u001b[0m, in \u001b[0;36mcreate_graph\u001b[1;34m(expression_matrix, gene_names, tf_names, regressor_type, regressor_kwargs, client, target_genes, limit, include_meta, early_stop_window_length, repartition_multiplier, seed)\u001b[0m\n\u001b[0;32m    448\u001b[0m \u001b[38;5;66;03m# gather the DataFrames into one distributed DataFrame\u001b[39;00m\n\u001b[0;32m    449\u001b[0m all_links_df \u001b[38;5;241m=\u001b[39m from_delayed(delayed_link_dfs, meta\u001b[38;5;241m=\u001b[39m_GRN_SCHEMA)\n\u001b[1;32m--> 450\u001b[0m all_meta_df \u001b[38;5;241m=\u001b[39m \u001b[43mfrom_delayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdelayed_meta_dfs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m_META_SCHEMA\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    452\u001b[0m \u001b[38;5;66;03m# optionally limit the number of resulting regulatory links, descending by top importance\u001b[39;00m\n\u001b[0;32m    453\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m limit:\n",
      "File \u001b[1;32md:\\Coding\\comp_bio\\endometrial_analysis\\.venv\\Lib\\site-packages\\dask_expr\\io\\_delayed.py:118\u001b[0m, in \u001b[0;36mfrom_delayed\u001b[1;34m(dfs, meta, divisions, prefix, verify_meta)\u001b[0m\n\u001b[0;32m    115\u001b[0m     dfs \u001b[38;5;241m=\u001b[39m [dfs]\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(dfs) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m--> 118\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMust supply at least one delayed object\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    120\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    121\u001b[0m     meta \u001b[38;5;241m=\u001b[39m delayed(make_meta)(dfs[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mcompute()\n",
      "\u001b[1;31mTypeError\u001b[0m: Must supply at least one delayed object"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Call grnboost2 with the extracted matrix\n",
    "adjacencies = grnboost2(ex_matrix, tf_names=hs_tfs_list, verbose=True, client_or_address=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "adjacencies = grnboost2(ex_matrix, tf_names=hs_tfs, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET_ID = \"HECA14039\"\n",
    "RESULTS_FOLDERNAME = \"output/results\"\n",
    "EXP_MTX_QC_FNAME = os.path.join(RESULTS_FOLDERNAME, '{}.qc.tpm.csv'.format(DATASET_ID))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pyscenic grn {EXP_MTX_QC_FNAME} {HUMAN_TFS_FNAME} -o {ADJACENCIES_FNAME} --num_workers 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "db_fnames = glob.glob(DATABASES_GLOB)\n",
    "def name(fname):\n",
    "    return os.path.splitext(os.path.basename(fname))[0]\n",
    "dbs = [RankingDatabase(fname=fname, name=name(fname)) for fname in db_fnames]\n",
    "dbs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 1: IDENTIFY TF with arboreto \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_tfs = \"/ddn1/vol1/staging/leuven/stg_00002/lcb/cflerin/resources/allTFs_hg38.txt\"\n",
    "\n",
    "!pyscenic grn {f_loom_path_scenic} {f_tfs} -o adj.csv --num_workers 20\n",
    "\n",
    "adjacencies = pd.read_csv(\"adj.tsv\", index_col=False, sep='\\t')\n",
    "adjacencies.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 2: Refine regulons through pruning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ranking databases\n",
    "f_db_glob = \"/ddn1/vol1/staging/leuven/res_00001/databases/cistarget/databases/homo_sapiens/hg38/refseq_r80/mc9nr/gene_based/*feather\"\n",
    "f_db_names = ' '.join( glob.glob(f_db_glob) )\n",
    "\n",
    "# motif databases\n",
    "f_motif_path = \"/ddn1/vol1/staging/leuven/res_00001/databases/cistarget/motif2tf/motifs-v9-nr.hgnc-m0.001-o0.0.tbl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pyscenic ctx adj.tsv \\\n",
    "    {f_db_names} \\\n",
    "    --annotations_fname {f_motif_path} \\\n",
    "    --expression_mtx_fname {f_loom_path_scenic} \\\n",
    "    --output reg.csv \\\n",
    "    --mask_dropouts \\\n",
    "    --num_workers 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nGenesDetectedPerCell = np.sum(adata.X>0, axis=1)\n",
    "percentiles = nGenesDetectedPerCell.quantile([.01, .05, .10, .50, 1])\n",
    "print(percentiles)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8, 5), dpi=150)\n",
    "sns.distplot(nGenesDetectedPerCell, norm_hist=False, kde=False, bins='fd')\n",
    "for i,x in enumerate(percentiles):\n",
    "    fig.gca().axvline(x=x, ymin=0,ymax=1, color='red')\n",
    "    ax.text(x=x, y=ax.get_ylim()[1], s=f'{int(x)} ({percentiles.index.values[i]*100}%)', color='red', rotation=30, size='x-small',rotation_mode='anchor' )\n",
    "ax.set_xlabel('# of genes')\n",
    "ax.set_ylabel('# of cells')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "powershell"
    }
   },
   "outputs": [],
   "source": [
    "!pyscenic aucell \\\n",
    "    {f_loom_path_scenic} \\\n",
    "    reg.csv \\\n",
    "    --output {f_pyscenic_output} \\\n",
    "    --num_workers 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### STEP 3: Cluster cells around regulons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = lp.connect( f_pyscenic_output, mode='r+', validate=False )\n",
    "auc_mtx = pd.DataFrame( lf.ca.RegulonsAUC, index=lf.ca.CellID)\n",
    "lf.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "runUmap = umap.UMAP(n_neighbors=10, min_dist=0.4, metric='correlation').fit_transform\n",
    "dr_umap = runUmap( auc_mtx )\n",
    "pd.DataFrame(dr_umap, columns=['X', 'Y'], index=auc_mtx.index).to_csv( \"scenic_umap.txt\", sep='\\t')\n",
    "# tSNE\n",
    "tsne = TSNE( n_jobs=20 )\n",
    "dr_tsne = tsne.fit_transform( auc_mtx )\n",
    "pd.DataFrame(dr_tsne, columns=['X', 'Y'], index=auc_mtx.index).to_csv( \"scenic_tsne.txt\", sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### STEP 4 Integrate the Output --- THIS LOOKS PROBLEMATIC AND VERY NOT RIGHT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# scenic output\n",
    "lf = lp.connect( f_pyscenic_output, mode='r+', validate=False )\n",
    "meta = json.loads(zlib.decompress(base64.b64decode( lf.attrs.MetaData )))\n",
    "#exprMat = pd.DataFrame( lf[:,:], index=lf.ra.Gene, columns=lf.ca.CellID)\n",
    "auc_mtx = pd.DataFrame( lf.ca.RegulonsAUC, index=lf.ca.CellID)\n",
    "regulons = lf.ra.Regulons\n",
    "dr_umap = pd.read_csv( 'scenic_umap.txt', sep='\\t', header=0, index_col=0 )\n",
    "dr_tsne = pd.read_csv( 'scenic_tsne.txt', sep='\\t', header=0, index_col=0 )\n",
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "auc_mtx.columns = auc_mtx.columns.str.replace('\\(','_(')\n",
    "regulons.dtype.names = tuple( [ x.replace(\"(\",\"_(\") for x in regulons.dtype.names ] )\n",
    "# regulon thresholds\n",
    "rt = meta['regulonThresholds']\n",
    "for i,x in enumerate(rt):\n",
    "    tmp = x.get('regulon').replace(\"(\",\"_(\")\n",
    "    x.update( {'regulon': tmp} )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsneDF = pd.DataFrame(adata.obsm['X_tsne'], columns=['_X', '_Y'])\n",
    "\n",
    "Embeddings_X = pd.DataFrame( index=lf.ca.CellID )\n",
    "Embeddings_X = pd.concat( [\n",
    "        pd.DataFrame(adata.obsm['X_umap'],index=adata.obs.index)[0] ,\n",
    "        pd.DataFrame(adata.obsm['X_pca'],index=adata.obs.index)[0] ,\n",
    "        dr_tsne['X'] ,\n",
    "        dr_umap['X']\n",
    "    ], sort=False, axis=1, join='outer' )\n",
    "Embeddings_X.columns = ['1','2','3','4']\n",
    "\n",
    "Embeddings_Y = pd.DataFrame( index=lf.ca.CellID )\n",
    "Embeddings_Y = pd.concat( [\n",
    "        pd.DataFrame(adata.obsm['X_umap'],index=adata.obs.index)[1] ,\n",
    "        pd.DataFrame(adata.obsm['X_pca'],index=adata.obs.index)[1] ,\n",
    "        dr_tsne['Y'] ,\n",
    "        dr_umap['Y']\n",
    "    ], sort=False, axis=1, join='outer' )\n",
    "Embeddings_Y.columns = ['1','2','3','4']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
